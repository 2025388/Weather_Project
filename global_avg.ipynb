{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9489a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import cfgrib\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from functools import reduce\n",
    "import datetime as dt\n",
    "import multiprocessing\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import csv  \n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd8103f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 15:11:49,853 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading prueba_era5.grib...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 15:11:50,585 INFO Request ID is ebecea7e-8c6a-40ed-8985-f0888e81073b\n",
      "2025-05-29 15:11:50,673 INFO status has been updated to accepted\n",
      "2025-05-29 15:11:59,178 INFO status has been updated to running\n",
      "2025-05-29 15:12:04,368 INFO status has been updated to successful\n",
      "c:\\Users\\dmoli\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\cfgrib\\xarray_plugin.py:131: FutureWarning: In a future version of xarray decode_timedelta will default to False rather than None. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "c:\\Users\\dmoli\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\cfgrib\\xarray_plugin.py:131: FutureWarning: In a future version of xarray decode_timedelta will default to False rather than None. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "c:\\Users\\dmoli\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\cfgrib\\xarray_plugin.py:131: FutureWarning: In a future version of xarray decode_timedelta will default to False rather than None. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "c:\\Users\\dmoli\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\cfgrib\\xarray_plugin.py:131: FutureWarning: In a future version of xarray decode_timedelta will default to False rather than None. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "c:\\Users\\dmoli\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\cfgrib\\xarray_plugin.py:131: FutureWarning: In a future version of xarray decode_timedelta will default to False rather than None. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "c:\\Users\\dmoli\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\cfgrib\\xarray_plugin.py:131: FutureWarning: In a future version of xarray decode_timedelta will default to False rather than None. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "c:\\Users\\dmoli\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\cfgrib\\xarray_plugin.py:131: FutureWarning: In a future version of xarray decode_timedelta will default to False rather than None. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "c:\\Users\\dmoli\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\cfgrib\\xarray_plugin.py:131: FutureWarning: In a future version of xarray decode_timedelta will default to False rather than None. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "c:\\Users\\dmoli\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\cfgrib\\xarray_plugin.py:131: FutureWarning: In a future version of xarray decode_timedelta will default to False rather than None. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n"
     ]
    }
   ],
   "source": [
    "client = cdsapi.Client()\n",
    "grib_file = f\"prueba_era5.grib\"\n",
    "dataset = \"reanalysis-era5-single-levels\"\n",
    "request = {\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"variable\": [\n",
    "        \"2m_dewpoint_temperature\",\n",
    "        \"2m_temperature\",\n",
    "        \"sea_surface_temperature\",\n",
    "        \"total_column_water_vapour\",\n",
    "        \"skin_temperature\",\n",
    "        \"mean_total_precipitation_rate\",\n",
    "        \"total_cloud_cover\",\n",
    "        \"runoff\",\n",
    "        \"surface_runoff\"\n",
    "    ],\n",
    "    \"year\": [\"1940\"],\n",
    "    \"month\": [\"01\"],\n",
    "    \"day\": [\"01\"],\n",
    "    \"time\": [\n",
    "        \"00:00\", \"01:00\", \"02:00\",\n",
    "        \"03:00\", \"04:00\", \"05:00\",\n",
    "        \"06:00\", \"07:00\", \"08:00\",\n",
    "        \"09:00\", \"10:00\", \"11:00\",\n",
    "        \"12:00\", \"13:00\", \"14:00\",\n",
    "        \"15:00\", \"16:00\", \"17:00\",\n",
    "        \"18:00\", \"19:00\", \"20:00\",\n",
    "        \"21:00\", \"22:00\", \"23:00\"\n",
    "    ],\n",
    "    \"data_format\": \"grib\",\n",
    "    \"download_format\": \"unarchived\",\n",
    "    \"area\": [90, -180, -90, 180],\n",
    "    'grid:': [1.0, 1.0],\n",
    "}\n",
    "# Download GRIB file\n",
    "print(f\"Downloading {grib_file}...\")\n",
    "client.retrieve(dataset, request).download(grib_file)\n",
    "# Open GRIB file\n",
    "dataset = cfgrib.open_datasets(grib_file)\n",
    "    # Initialize a list to store DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9344e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfs = []\n",
    "\n",
    "for ds in dataset:\n",
    "    # Try to drop 'step' and other multi-dim coords if needed\n",
    "    if \"step\" in ds.dims:\n",
    "        ds = ds.mean(dim=\"step\")  # Or choose step=0 or another value\n",
    "\n",
    "    if \"valid_time\" in ds.coords:\n",
    "        ds = ds.drop_vars(\"valid_time\")\n",
    "\n",
    "    if \"number\" in ds.coords:\n",
    "        ds = ds.drop_vars(\"number\")\n",
    "\n",
    "    try:\n",
    "        df = ds.to_dataframe().reset_index()\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed converting a dataset: {e}\")\n",
    "\n",
    "# Merge all DataFrames on ['time', 'latitude', 'longitude']\n",
    "if dfs:\n",
    "    merged_df = reduce(lambda left, right: pd.merge(\n",
    "        left, right,\n",
    "        on=[\"time\", \"latitude\", \"longitude\"],\n",
    "        how=\"outer\",\n",
    "        suffixes=('', '_dup')  # Prevent merge error on duplicate names\n",
    "    ), dfs)\n",
    "\n",
    "    # Drop duplicated columns if created\n",
    "    merged_df = merged_df.loc[:, ~merged_df.columns.str.endswith('_dup')]\n",
    "\n",
    "else:\n",
    "    merged_df = pd.DataFrame()\n",
    "    print(\"No dataframes were created from the datasets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecb9792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = merged_df.copy()\n",
    "dfm['coordinates'] = dfm['latitude'].astype(str) + ',' + dfm['longitude'].astype(str)\n",
    "dfm['time'] = pd.to_datetime(dfm['time'], format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "dfm['day'] = dfm['time'].dt.day\n",
    "dfm['month'] = dfm['time'].dt.month\n",
    "dfm['year'] = dfm['time'].dt.year\n",
    "dfm['hour'] = dfm['time'].dt.hour\n",
    "dfm['day of the year'] = dfm['time'].dt.dayofyear\n",
    "new_order = ['time', 'day of the year', 'day', 'month', 'year', 'hour', 'latitude', 'longitude', 'coordinates'] + [col for col in dfm.columns if col not in ['time', 'day', 'month', 'year', 'hour', 'latitude', 'longitude', 'coordinates']]\n",
    "dfm = dfm[new_order]\n",
    "#drop date if it comes from different day\n",
    "dfm.sort_values(by='time')\n",
    "if dfm['day'].iloc[0] != dfm['day'].iloc[-1]:\n",
    "    dfm.dropna(subset=['t2m'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "860fbbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>day of the year</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>hour</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>surface</th>\n",
       "      <th>...</th>\n",
       "      <th>ro</th>\n",
       "      <th>avg_tprate</th>\n",
       "      <th>step</th>\n",
       "      <th>sst</th>\n",
       "      <th>tcwv</th>\n",
       "      <th>tcc</th>\n",
       "      <th>t2m</th>\n",
       "      <th>d2m</th>\n",
       "      <th>skt</th>\n",
       "      <th>day of the year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1940-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1940</td>\n",
       "      <td>0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>-90.0,-180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.756644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>241.756897</td>\n",
       "      <td>237.667786</td>\n",
       "      <td>240.96019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1940-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1940</td>\n",
       "      <td>0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>-179.0</td>\n",
       "      <td>-90.0,-179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.756644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>241.756897</td>\n",
       "      <td>237.667786</td>\n",
       "      <td>240.96019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1940-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1940</td>\n",
       "      <td>0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>-178.0</td>\n",
       "      <td>-90.0,-178.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.756644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>241.756897</td>\n",
       "      <td>237.667786</td>\n",
       "      <td>240.96019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1940-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1940</td>\n",
       "      <td>0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>-177.0</td>\n",
       "      <td>-90.0,-177.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.756644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>241.756897</td>\n",
       "      <td>237.667786</td>\n",
       "      <td>240.96019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1940-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1940</td>\n",
       "      <td>0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>-176.0</td>\n",
       "      <td>-90.0,-176.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.756644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>241.756897</td>\n",
       "      <td>237.667786</td>\n",
       "      <td>240.96019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time  day of the year  day  month  year  hour  latitude  longitude  \\\n",
       "0 1940-01-01                1    1      1  1940     0     -90.0     -180.0   \n",
       "1 1940-01-01                1    1      1  1940     0     -90.0     -179.0   \n",
       "2 1940-01-01                1    1      1  1940     0     -90.0     -178.0   \n",
       "3 1940-01-01                1    1      1  1940     0     -90.0     -177.0   \n",
       "4 1940-01-01                1    1      1  1940     0     -90.0     -176.0   \n",
       "\n",
       "    coordinates  surface  ...  ro  avg_tprate   step sst      tcwv  tcc  \\\n",
       "0  -90.0,-180.0      NaN  ... NaN         NaN 0 days NaN  0.756644  0.0   \n",
       "1  -90.0,-179.0      NaN  ... NaN         NaN 0 days NaN  0.756644  0.0   \n",
       "2  -90.0,-178.0      NaN  ... NaN         NaN 0 days NaN  0.756644  0.0   \n",
       "3  -90.0,-177.0      NaN  ... NaN         NaN 0 days NaN  0.756644  0.0   \n",
       "4  -90.0,-176.0      NaN  ... NaN         NaN 0 days NaN  0.756644  0.0   \n",
       "\n",
       "          t2m         d2m        skt  day of the year  \n",
       "0  241.756897  237.667786  240.96019                1  \n",
       "1  241.756897  237.667786  240.96019                1  \n",
       "2  241.756897  237.667786  240.96019                1  \n",
       "3  241.756897  237.667786  240.96019                1  \n",
       "4  241.756897  237.667786  240.96019                1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27d5ce9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.033176e+06\n",
       "mean     2.864469e+02\n",
       "std      1.155671e+01\n",
       "min      2.714600e+02\n",
       "25%      2.734619e+02\n",
       "50%      2.874014e+02\n",
       "75%      2.981240e+02\n",
       "max      3.048613e+02\n",
       "Name: sst, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm['sst'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "240f3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log path\n",
    "log_path = r\"C:\\Users\\dmoli\\Documents\\Coding\\Weathercast_project\\date_log_global.csv\"\n",
    "dfm['time'] = pd.to_datetime(dfm['time'])\n",
    "dfm = dfm.sort_values(['coordinates', 'time'])\n",
    "\n",
    "# Start tracking broken columns\n",
    "broken_columns = []\n",
    "\n",
    "# Start log list\n",
    "log_entries = []\n",
    "\n",
    "# Check missing data\n",
    "missing_data = dfm.isnull()\n",
    "\n",
    "for column in dfm.columns:\n",
    "    missing_count = missing_data[column].sum()\n",
    "    if isinstance(missing_count, pd.Series):\n",
    "        missing_count = missing_count.sum()\n",
    "    missing_count = int(missing_count)\n",
    "\n",
    "    if missing_count > 0:\n",
    "        timestamp = dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Columns that can tolerate up to 3 full missing days\n",
    "        relaxed_cols = [\n",
    "            'tcwv','t2m', 'd2m', 'skt', 'tcc' \n",
    "                                    ]\n",
    "\n",
    "        if column not in relaxed_cols:\n",
    "            if missing_count > 1433520:\n",
    "                log_entries.append([timestamp, column, missing_count, 'too many missing values'])\n",
    "                broken_columns.append(column)\n",
    "        else:\n",
    "            ratio = round(missing_count / 65160, 2)\n",
    "            if ratio > 3:\n",
    "                log_entries.append([timestamp, column, missing_count, f'missing > 3 days ({ratio})'])\n",
    "                broken_columns.append(column)\n",
    "\n",
    "# Save log if needed\n",
    "if log_entries:\n",
    "    log_df = pd.DataFrame(log_entries, columns=[\"timestamp\", \"variable\", \"missing_count\", \"note\"])\n",
    "    if os.path.exists(log_path):\n",
    "        log_df.to_csv(log_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        log_df.to_csv(log_path, index=False)\n",
    "\n",
    "# ------- Now: Handle 2-3 hours missing data for specific columns ------- #\n",
    "\n",
    "# Columns that need special NaN handling\n",
    "target_columns = [\n",
    "            'tcwv','t2m', 'd2m', 'skt', 'tcc'\n",
    "                                    ]\n",
    "\n",
    "\n",
    "\n",
    "# Group by coordinate\n",
    "for coord, group in dfm.groupby('coordinates'):\n",
    "    for col in target_columns:\n",
    "        # Select time and the target variable\n",
    "        sub_df = group[['time', col]].set_index('time')\n",
    "\n",
    "        # Find missing values\n",
    "        nan_mask = sub_df[col].isna()\n",
    "        if nan_mask.sum() > 0:\n",
    "            nan_times = sub_df[nan_mask].index\n",
    "\n",
    "            for t in nan_times:\n",
    "                # Define +/- 2 hours window\n",
    "                prev_2h = t - pd.Timedelta(hours=2)\n",
    "                next_2h = t + pd.Timedelta(hours=2)\n",
    "\n",
    "                window = sub_df.loc[prev_2h:next_2h][col]\n",
    "\n",
    "                # If 5 expected timestamps and max 2 missing, proceed\n",
    "                if len(window) == 5 and window.isna().sum() <= 2:\n",
    "                    # Check if missing hours are consecutive\n",
    "                    missing_consec = window.isna().astype(int).diff().abs().sum() == 2\n",
    "\n",
    "                    if missing_consec:\n",
    "                        # Interpolate over window\n",
    "                        dfm.loc[(dfm['coordinates'] == coord) & (dfm['time'].between(prev_2h, next_2h)), col] = \\\n",
    "                            dfm.loc[(dfm['coordinates'] == coord) & (dfm['time'].between(prev_2h, next_2h)), col].interpolate(method='linear', limit_direction='both')\n",
    "                    else:\n",
    "                        # Take average of 2 hours before and 2 hours after\n",
    "                        try:\n",
    "                            before = sub_df.loc[[prev_2h, t - pd.Timedelta(hours=1)]][col].dropna()\n",
    "                            after = sub_df.loc[[t + pd.Timedelta(hours=1), next_2h]][col].dropna()\n",
    "                            if len(before) == 2 and len(after) == 2:\n",
    "                                avg_val = pd.concat([before, after]).mean()\n",
    "                                dfm.loc[(dfm['coordinates'] == coord) & (dfm['time'] == t), col] = avg_val\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error filling value for {coord} at {t}:\", e)\n",
    "                            continue\n",
    "low_freq_columns = [\n",
    "    'sro','ro', 'sst', 'avg_tprate'\n",
    "]\n",
    "\n",
    "# Sort first to ensure time continuity\n",
    "dfm = dfm.sort_values(['coordinates', 'time'])\n",
    "\n",
    "# Interpolate low-frequency columns within each coordinate group\n",
    "for col in low_freq_columns:\n",
    "    dfm[col] = dfm.groupby('coordinates')[col].transform(\n",
    "        lambda group: group.interpolate(method='linear', limit_direction='both'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68526215",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # process information to be stored in the database\n",
    "    \n",
    "\n",
    "    dfm['t2m'] = (dfm['t2m'] - 273.15).round(2)\n",
    "    dfm['d2m'] = (dfm['d2m'] - 273.15).round(2)\n",
    "    dfm['skt'] = (dfm['skt'] - 273.15).round(2)\n",
    "    \n",
    "    #identify outliers and store in DB\n",
    "    # Initialize list to collect outliers\n",
    "    outliers_list = []\n",
    "\n",
    "    # Exclude metadata columns\n",
    "    meta_cols = ['time', 'day of the year', 'day', 'month', 'year', 'hour', 'coordinates']\n",
    "    data_cols = [col for col in dfm.columns if col not in meta_cols]\n",
    "\n",
    "    # Group by coordinate\n",
    "    for coord, group in dfm.groupby('coordinates'):\n",
    "        for col in data_cols:\n",
    "            if group[col].isnull().all():\n",
    "                continue  # skip all-NaN columns\n",
    "\n",
    "            try:\n",
    "                # Compute z-scores\n",
    "                col_data = group[col].dropna()\n",
    "                if col_data.std() < 1e-8:\n",
    "                    continue  # skip nearly constant series``\n",
    "                z_scores = zscore(col_data)\n",
    "                outlier_mask = np.abs(z_scores) > 3\n",
    "\n",
    "                # Get times and values\n",
    "                outlier_values = group.loc[group[col].dropna().index[outlier_mask], ['time', col]]\n",
    "                for _, row in outlier_values.iterrows():\n",
    "                    outliers_list.append({\n",
    "                        'coordinates': coord,\n",
    "                        'date': row['time'].date(),\n",
    "                        'time': row['time'].time(),\n",
    "                        'value': row[col],\n",
    "                        'variable': col\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error computing outliers for {coord}, {col}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    outliers_df = pd.DataFrame(outliers_list)\n",
    "\n",
    "    # Save to DB\n",
    "    \n",
    "    # MySQL Connection\n",
    "    user = 'root'\n",
    "    password = 'Hamilton1186!'\n",
    "    host = '127.0.0.1'\n",
    "    port = '3306'\n",
    "    db = 'weatherdb'\n",
    "    table_name = 'weather_summary_global_outliers'\n",
    "\n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_engine(f\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\")\n",
    "\n",
    "    # Check if table exists\n",
    "    inspector = inspect(engine)\n",
    "    if not inspector.has_table(table_name):\n",
    "        # Create table by writing empty df with same schema\n",
    "        outliers_df.head(0).to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "        print(f\"Table '{table_name}' created.\")\n",
    "    else:\n",
    "        print(f\"Table '{table_name}' already exists. Skipping creation.\")\n",
    "\n",
    "    # Append data\n",
    "    outliers_df.to_sql(name=table_name, con=engine, if_exists='append', index=False)\n",
    "    print(f\"Data inserted into '{table_name}'.\")\n",
    "\n",
    "    # Dispose of the engine to close the connection\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48245e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  day_of_year  tcwv_mean  tcwv_min   tcwv_max   tcwv_std  \\\n",
      "0 1940-01-01            1  15.919818  0.148484  66.949211  15.349381   \n",
      "\n",
      "   tcwv_median  tcwv_<lambda>  t2m_mean  t2m_min  ...  avg_tprate_median  \\\n",
      "0    10.187007      66.800728  2.791903   -49.16  ...           0.000002   \n",
      "\n",
      "   avg_tprate_<lambda>  coord_min_temp  coord_max_temp  coord_min_time  \\\n",
      "0             0.004132       73.0,97.0     -21.0,119.0              21   \n",
      "\n",
      "   coord_max_time  ocean_min_temp  ocean_max_temp  ocean_min_time  \\\n",
      "0               6     -78.0,-44.0     -15.0,136.0               0   \n",
      "\n",
      "   ocean_max_time  \n",
      "0              22  \n",
      "\n",
      "[1 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "weather = [ 't2m', 'd2m', 'sst', 'skt', 'tcc', 'sro', 'ro', 'avg_tprate', 'tcwv']\n",
    "\n",
    "common_stats = ['mean', 'min', 'max', 'std', 'median', lambda x: x.max() - x.min()]\n",
    "\n",
    "summary_data = {}\n",
    "for weather_var in weather:\n",
    "    for stat in common_stats:\n",
    "        weather_cast = dfm[weather_var].agg(stat)\n",
    "        key = f\"{weather_var}_{stat.__name__}\" if callable(stat) else f\"{weather_var}_{stat}\"\n",
    "        summary_data[key] = weather_cast\n",
    "            \n",
    "\n",
    "summary_df = pd.DataFrame([summary_data])\n",
    "summary_df['Date'] = dfm['time'].iloc[0]\n",
    "summary_df['Date'] = pd.to_datetime(summary_df['Date'])\n",
    "summary_df['day_of_year'] = summary_df['Date'].dt.dayofyear\n",
    "summary_df['coord_min_temp'] = dfm.loc[dfm['t2m'].idxmin(), 'coordinates']\n",
    "summary_df['coord_max_temp'] = dfm.loc[dfm['t2m'].idxmax(), 'coordinates']\n",
    "summary_df['coord_min_time'] = dfm.loc[dfm['t2m'].idxmin(), 'hour']\n",
    "summary_df['coord_max_time'] = dfm.loc[dfm['t2m'].idxmax(), 'hour']\n",
    "summary_df['ocean_min_temp'] = dfm.loc[dfm['sst'].idxmin(), 'coordinates']\n",
    "summary_df['ocean_max_temp'] = dfm.loc[dfm['sst'].idxmax(), 'coordinates']\n",
    "summary_df['ocean_min_time'] = dfm.loc[dfm['sst'].idxmin(), 'hour']\n",
    "summary_df['ocean_max_time'] = dfm.loc[dfm['sst'].idxmax(), 'hour']\n",
    "new_order = ['Date', 'day_of_year'] + [col for col in summary_df.columns if col not in ['Date', 'day_of_year']]\n",
    "summary_df = summary_df[new_order]\n",
    "\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b20fd8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'weather_summary_global' already exists. Skipping creation.\n",
      "Data inserted into 'weather_summary_global'.\n",
      "DataFrame successfully stored in the MySQL database!\n"
     ]
    }
   ],
   "source": [
    "# Save to DB\n",
    "# MySQL Connection\n",
    "user = 'root'\n",
    "password = 'Hamilton1186!'\n",
    "host = '127.0.0.1'\n",
    "port = '3306'\n",
    "db = 'weatherdb'\n",
    "table_name = 'weather_summary_global'\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(f\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\")\n",
    "\n",
    "# Check if table exists\n",
    "inspector = inspect(engine)\n",
    "if not inspector.has_table(table_name):\n",
    "    # Create table by writing empty df with same schema\n",
    "    summary_df.head(0).to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "    print(f\"Table '{table_name}' created.\")\n",
    "else:\n",
    "    print(f\"Table '{table_name}' already exists. Skipping creation.\")\n",
    "\n",
    "# Append data\n",
    "summary_df.to_sql(name=table_name, con=engine, if_exists='append', index=False)\n",
    "print(f\"Data inserted into '{table_name}'.\")\n",
    "\n",
    "# Dispose of the engine to close the connection\n",
    "engine.dispose()\n",
    "# Delete GRIB file if it exists\n",
    "if os.path.exists(grib_file):\n",
    "    try:\n",
    "        os.remove(grib_file)\n",
    "        print(f\"Deleted {grib_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {grib_file}: {e}\")\n",
    "\n",
    "print(\"DataFrame successfully stored in the MySQL database!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a88e66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'weather_summary_global' already exists. Skipping creation.\n",
      "Data inserted into 'weather_summary_global'.\n",
      "DataFrame successfully stored in the MySQL database!\n"
     ]
    }
   ],
   "source": [
    "# Save to DB\n",
    "# MySQL Connection\n",
    "user = 'root'\n",
    "password = 'Hamilton1186!'\n",
    "host = '127.0.0.1'\n",
    "port = '3306'\n",
    "db = 'weatherdb'\n",
    "table_name = 'weather_summary_global'\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(f\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\")\n",
    "\n",
    "# Check if table exists\n",
    "inspector = inspect(engine)\n",
    "if not inspector.has_table(table_name):\n",
    "    # Create table by writing empty df with same schema\n",
    "    summary_df.head(0).to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "    print(f\"Table '{table_name}' created.\")\n",
    "else:\n",
    "    print(f\"Table '{table_name}' already exists. Skipping creation.\")\n",
    "\n",
    "# Append data\n",
    "summary_df.to_sql(name=table_name, con=engine, if_exists='append', index=False)\n",
    "print(f\"Data inserted into '{table_name}'.\")\n",
    "\n",
    "# Dispose of the engine to close the connection\n",
    "engine.dispose()\n",
    "# Delete GRIB file if it exists\n",
    "if os.path.exists(grib_file):\n",
    "    try:\n",
    "        os.remove(grib_file)\n",
    "        print(f\"Deleted {grib_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {grib_file}: {e}\")\n",
    "\n",
    "print(\"DataFrame successfully stored in the MySQL database!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
